# Backend ê°œë°œ ê°€ì´ë“œ

MLOps Workflow Backend ê°œë°œì„ ìœ„í•œ ì‹¤ìš©ì ì¸ ê°€ì´ë“œ

## ë¹ ë¥¸ ì‹œìž‘

### ë¡œì»¬ ê°œë°œ í™˜ê²½ êµ¬ì¶• (3ë¶„)

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
cd backend
python -m venv venv

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™”
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cat > .env << EOF
DATABASE_URL=postgresql://airflow:airflow@localhost:5432/airflow
AIRFLOW_API_URL=http://localhost:8080/api/v1
AIRFLOW_USERNAME=admin
AIRFLOW_PASSWORD=admin
DAGS_FOLDER=../dags
DEBUG=True
EOF

# 5. ê°œë°œ ì„œë²„ ì‹¤í–‰ (PostgreSQLì´ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨)
uvicorn app.main:app --reload
```

**ì ‘ì†:** http://localhost:8000/docs

---

## ì¼ë°˜ì ì¸ ê°œë°œ ìž‘ì—…

### 1. ìƒˆ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

**ì‹œë‚˜ë¦¬ì˜¤:** "Workflowì— íƒœê·¸(tags) ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê³  ì‹¶ì–´ìš”"

#### Step 1: ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ìˆ˜ì •

`app/models/workflow.py`:
```python
from sqlalchemy import Column, ARRAY, String
from sqlalchemy.dialects.postgresql import UUID

class Workflow(Base):
    __tablename__ = "workflows"

    # ... ê¸°ì¡´ í•„ë“œë“¤ ...

    # ìƒˆ í•„ë“œ ì¶”ê°€
    tags = Column(ARRAY(String), default=list)
```

#### Step 2: ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„± ë° ì ìš©

```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ ìžë™ ìƒì„±
alembic revision --autogenerate -m "Add tags to workflows"

# ìƒì„±ëœ íŒŒì¼ í™•ì¸
ls alembic/versions/

# ë§ˆì´ê·¸ë ˆì´ì…˜ ì ìš©
alembic upgrade head
```

#### Step 3: Pydantic ìŠ¤í‚¤ë§ˆ ìˆ˜ì •

`app/schemas/workflow.py`:
```python
from typing import Optional, List

class WorkflowCreate(BaseModel):
    name: str
    description: Optional[str] = None
    schedule: Optional[str] = None
    is_active: bool = True
    tags: List[str] = []  # ì¶”ê°€

class WorkflowResponse(BaseModel):
    id: str
    name: str
    # ... ê¸°ì¡´ í•„ë“œë“¤ ...
    tags: List[str]  # ì¶”ê°€

    class Config:
        from_attributes = True
```

#### Step 4: API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì • (í•„ìš”ì‹œ)

`app/api/v1/workflows.py`:
```python
# íƒœê·¸ë¡œ í•„í„°ë§í•˜ëŠ” ìƒˆ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@router.get("/by-tag/{tag}")
def get_workflows_by_tag(
    tag: str,
    db: Session = Depends(get_db)
):
    workflows = db.query(Workflow).filter(
        Workflow.tags.contains([tag])
    ).all()
    return workflows
```

#### Step 5: í…ŒìŠ¤íŠ¸

```bash
# API ë¬¸ì„œì—ì„œ í…ŒìŠ¤íŠ¸
# http://localhost:8000/docs

# ë˜ëŠ” curlë¡œ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/api/v1/workflows/ \
  -H "Content-Type: application/json" \
  -d '{
    "name": "tagged_workflow",
    "tags": ["ml", "production"]
  }'
```

---

### 2. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™ ì¶”ê°€

**ì‹œë‚˜ë¦¬ì˜¤:** "Slackìœ¼ë¡œ Job ì™„ë£Œ ì•Œë¦¼ì„ ë³´ë‚´ê³  ì‹¶ì–´ìš”"

#### Step 1: ì˜ì¡´ì„± ì¶”ê°€

`requirements.txt`:
```
slack-sdk==3.26.2
```

ì„¤ì¹˜:
```bash
pip install slack-sdk==3.26.2
```

#### Step 2: í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€

`.env`:
```env
SLACK_BOT_TOKEN=xoxb-your-slack-bot-token
SLACK_CHANNEL=#mlops-notifications
```

`app/core/config.py`:
```python
class Settings(BaseSettings):
    # ... ê¸°ì¡´ ì„¤ì • ...

    SLACK_BOT_TOKEN: Optional[str] = None
    SLACK_CHANNEL: str = "#mlops-notifications"
```

#### Step 3: ì„œë¹„ìŠ¤ í´ëž˜ìŠ¤ ìƒì„±

`app/services/slack_notifier.py`:
```python
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
from app.core.config import settings
from typing import Optional

class SlackNotifier:
    def __init__(self):
        if settings.SLACK_BOT_TOKEN:
            self.client = WebClient(token=settings.SLACK_BOT_TOKEN)
            self.enabled = True
        else:
            self.enabled = False

    def send_job_completion(
        self,
        workflow_name: str,
        job_id: str,
        status: str,
        duration: str
    ) -> bool:
        """Send job completion notification to Slack"""
        if not self.enabled:
            return False

        emoji = "âœ…" if status == "success" else "âŒ"
        message = (
            f"{emoji} *Workflow Completed*\n"
            f"â€¢ Workflow: `{workflow_name}`\n"
            f"â€¢ Job ID: `{job_id}`\n"
            f"â€¢ Status: {status}\n"
            f"â€¢ Duration: {duration}"
        )

        try:
            self.client.chat_postMessage(
                channel=settings.SLACK_CHANNEL,
                text=message
            )
            return True
        except SlackApiError as e:
            print(f"Slack API error: {e}")
            return False
```

#### Step 4: ì˜ì¡´ì„± í•¨ìˆ˜ ì¶”ê°€

`app/api/deps.py`:
```python
from app.services.slack_notifier import SlackNotifier

def get_slack_notifier():
    """Slack notifier dependency"""
    return SlackNotifier()
```

#### Step 5: APIì—ì„œ ì‚¬ìš©

`app/api/v1/jobs.py`:
```python
from app.services.slack_notifier import SlackNotifier
from app.api.deps import get_slack_notifier

@router.get("/{job_run_id}")
async def get_job_run(
    job_run_id: UUID,
    db: Session = Depends(get_db),
    airflow: AirflowClient = Depends(get_airflow_client),
    slack: SlackNotifier = Depends(get_slack_notifier)  # ì¶”ê°€
):
    # ... ê¸°ì¡´ ì½”ë“œ ...

    # Jobì´ ì™„ë£Œë˜ë©´ Slack ì•Œë¦¼
    if job_run.status in ["success", "failed"]:
        duration = str(job_run.ended_at - job_run.started_at)
        slack.send_job_completion(
            workflow_name=workflow.name,
            job_id=str(job_run.id),
            status=job_run.status,
            duration=duration
        )

    return job_run
```

---

### 3. ë°ì´í„° ê²€ì¦ ë¡œì§ ì¶”ê°€

**ì‹œë‚˜ë¦¬ì˜¤:** "Workflow ì´ë¦„ì€ ì˜ë¬¸ìžë¡œ ì‹œìž‘í•˜ê³  ì˜ë¬¸ìž, ìˆ«ìž, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš©í•˜ê³  ì‹¶ì–´ìš”"

#### Pydantic Validator ì‚¬ìš©

`app/schemas/workflow.py`:
```python
from pydantic import BaseModel, Field, field_validator
import re

class WorkflowCreate(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    description: Optional[str] = None
    schedule: Optional[str] = None
    is_active: bool = True

    @field_validator('name')
    @classmethod
    def validate_name(cls, v: str) -> str:
        """Validate workflow name format"""
        if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', v):
            raise ValueError(
                'Name must start with a letter and contain only '
                'letters, numbers, and underscores'
            )
        return v

    @field_validator('schedule')
    @classmethod
    def validate_schedule(cls, v: Optional[str]) -> Optional[str]:
        """Validate cron schedule"""
        if v is None:
            return v

        # í”„ë¦¬ì…‹ í—ˆìš©
        presets = ['@once', '@hourly', '@daily', '@weekly', '@monthly']
        if v in presets:
            return v

        # Cron í‘œí˜„ì‹ ê²€ì¦ (ê°„ë‹¨í•œ ì˜ˆì œ)
        parts = v.split()
        if len(parts) != 5:
            raise ValueError(
                'Invalid cron expression. Use presets like @daily '
                'or cron format: "* * * * *"'
            )

        return v
```

---

### 4. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì„œë¹„ìŠ¤ ì¶”ê°€

**ì‹œë‚˜ë¦¬ì˜¤:** "Workflow ë³µì œ(clone) ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê³  ì‹¶ì–´ìš”"

#### Step 1: ì„œë¹„ìŠ¤ í´ëž˜ìŠ¤ ìƒì„±

`app/services/workflow_service.py`:
```python
from sqlalchemy.orm import Session
from app.models.workflow import Workflow
from app.models.task import Task
from typing import Tuple
import uuid

class WorkflowService:
    @staticmethod
    def clone_workflow(
        db: Session,
        source_workflow_id: uuid.UUID,
        new_name: str
    ) -> Tuple[Workflow, list[Task]]:
        """Clone a workflow with all its tasks"""
        # ì›ë³¸ workflow ì¡°íšŒ
        source_workflow = db.query(Workflow).filter(
            Workflow.id == source_workflow_id
        ).first()

        if not source_workflow:
            raise ValueError(f"Workflow {source_workflow_id} not found")

        # ìƒˆ workflow ìƒì„±
        new_workflow = Workflow(
            name=new_name,
            description=f"Cloned from {source_workflow.name}",
            schedule=source_workflow.schedule,
            is_active=False  # ë³µì œë³¸ì€ ë¹„í™œì„±í™” ìƒíƒœë¡œ
        )
        db.add(new_workflow)
        db.flush()  # ID ìƒì„±

        # Taskë“¤ ë³µì œ
        source_tasks = db.query(Task).filter(
            Task.workflow_id == source_workflow_id
        ).all()

        new_tasks = []
        for source_task in source_tasks:
            new_task = Task(
                workflow_id=new_workflow.id,
                name=source_task.name,
                python_callable=source_task.python_callable,
                params=source_task.params,
                dependencies=source_task.dependencies,
                retry_count=source_task.retry_count,
                retry_delay=source_task.retry_delay
            )
            db.add(new_task)
            new_tasks.append(new_task)

        db.commit()
        db.refresh(new_workflow)

        return new_workflow, new_tasks
```

#### Step 2: API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

`app/api/v1/workflows.py`:
```python
from app.services.workflow_service import WorkflowService

@router.post("/{workflow_id}/clone")
def clone_workflow(
    workflow_id: UUID,
    new_name: str,
    db: Session = Depends(get_db)
):
    """Clone a workflow with all its tasks"""
    try:
        new_workflow, new_tasks = WorkflowService.clone_workflow(
            db, workflow_id, new_name
        )
        return {
            "workflow": new_workflow,
            "tasks_count": len(new_tasks),
            "message": f"Workflow cloned as '{new_name}'"
        }
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to clone workflow: {str(e)}"
        )
```

---

### 5. ë°±ê·¸ë¼ìš´ë“œ ìž‘ì—… ì¶”ê°€

**ì‹œë‚˜ë¦¬ì˜¤:** "ì˜¤ëž˜ëœ Job ì‹¤í–‰ ê¸°ë¡ì„ ìžë™ìœ¼ë¡œ ì •ë¦¬í•˜ê³  ì‹¶ì–´ìš”"

#### Step 1: ë°±ê·¸ë¼ìš´ë“œ ìž‘ì—… í•¨ìˆ˜ ìƒì„±

`app/services/cleanup_service.py`:
```python
from sqlalchemy.orm import Session
from app.models.job_run import JobRun
from datetime import datetime, timedelta
from app.core.database import SessionLocal

class CleanupService:
    @staticmethod
    def cleanup_old_job_runs(days: int = 30) -> int:
        """Delete job runs older than specified days"""
        db = SessionLocal()
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=days)

            deleted_count = db.query(JobRun).filter(
                JobRun.created_at < cutoff_date
            ).delete()

            db.commit()
            return deleted_count
        finally:
            db.close()
```

#### Step 2: FastAPI ë°±ê·¸ë¼ìš´ë“œ ìž‘ì—…ìœ¼ë¡œ ì‹¤í–‰

`app/api/v1/monitoring.py`:
```python
from fastapi import BackgroundTasks
from app.services.cleanup_service import CleanupService

@router.post("/cleanup")
def trigger_cleanup(
    background_tasks: BackgroundTasks,
    days: int = 30
):
    """Trigger background cleanup of old job runs"""
    background_tasks.add_task(
        CleanupService.cleanup_old_job_runs,
        days
    )
    return {
        "message": f"Cleanup task scheduled for records older than {days} days"
    }
```

#### Step 3: ì£¼ê¸°ì  ì‹¤í–‰ (ì„ íƒì‚¬í•­)

ìŠ¤ì¼€ì¤„ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©:

```bash
pip install apscheduler
```

`app/main.py`:
```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from app.services.cleanup_service import CleanupService

# ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
scheduler = AsyncIOScheduler()

@app.on_event("startup")
async def startup_event():
    # ë§¤ì¼ ìžì •ì— ì •ë¦¬ ìž‘ì—… ì‹¤í–‰
    scheduler.add_job(
        CleanupService.cleanup_old_job_runs,
        'cron',
        hour=0,
        minute=0,
        args=[30]  # 30ì¼ ì´ìƒëœ ê¸°ë¡ ì‚­ì œ
    )
    scheduler.start()

@app.on_event("shutdown")
async def shutdown_event():
    scheduler.shutdown()
```

---

## ë””ë²„ê¹… íŒ

### 1. ë¡œê¹… ì„¤ì •

`app/core/config.py`:
```python
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG if settings.DEBUG else logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)
```

ì‚¬ìš©:
```python
from app.core.config import logger

@router.post("/workflows/")
def create_workflow(workflow_in: WorkflowCreate, db: Session = Depends(get_db)):
    logger.info(f"Creating workflow: {workflow_in.name}")

    try:
        workflow = Workflow(**workflow_in.model_dump())
        db.add(workflow)
        db.commit()
        logger.info(f"Workflow created: {workflow.id}")
        return workflow
    except Exception as e:
        logger.error(f"Failed to create workflow: {e}")
        raise
```

### 2. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë””ë²„ê¹…

SQLAlchemy ì¿¼ë¦¬ ë¡œê¹…:

`app/core/database.py`:
```python
from sqlalchemy import create_engine

engine = create_engine(
    settings.DATABASE_URL,
    pool_pre_ping=True,
    echo=settings.DEBUG,  # Trueë©´ ëª¨ë“  SQL ì¿¼ë¦¬ ì¶œë ¥
)
```

### 3. Pydantic ê²€ì¦ ì—ëŸ¬ ìƒì„¸ ë³´ê¸°

```python
from pydantic import ValidationError

try:
    workflow = WorkflowCreate(**data)
except ValidationError as e:
    print(e.json())  # JSON í˜•ì‹ìœ¼ë¡œ ì—ëŸ¬ ì¶œë ¥
```

### 4. FastAPI ë””ë²„ê·¸ ëª¨ë“œ

`app/main.py`:
```python
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,  # ì½”ë“œ ë³€ê²½ ì‹œ ìžë™ ìž¬ì‹œìž‘
        log_level="debug"
    )
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”

**N+1 ì¿¼ë¦¬ ë¬¸ì œ í•´ê²°:**

```python
from sqlalchemy.orm import joinedload

# Bad: N+1 queries
workflows = db.query(Workflow).all()
for workflow in workflows:
    print(workflow.tasks)  # ê° workflowë§ˆë‹¤ ì¶”ê°€ ì¿¼ë¦¬ ë°œìƒ

# Good: Eager loading
workflows = db.query(Workflow).options(
    joinedload(Workflow.tasks)
).all()
for workflow in workflows:
    print(workflow.tasks)  # ì¶”ê°€ ì¿¼ë¦¬ ì—†ìŒ
```

### 2. íŽ˜ì´ì§€ë„¤ì´ì…˜

```python
from typing import Optional

@router.get("/workflows/")
def list_workflows(
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db)
):
    """List workflows with pagination"""
    total = db.query(Workflow).count()
    workflows = db.query(Workflow)\
        .offset(skip)\
        .limit(limit)\
        .all()

    return {
        "total": total,
        "items": workflows,
        "page": skip // limit + 1,
        "page_size": limit
    }
```

### 3. ìºì‹± (Redis)

```bash
pip install redis
```

`app/services/cache_service.py`:
```python
import redis
import json
from app.core.config import settings

class CacheService:
    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            decode_responses=True
        )

    def get(self, key: str):
        value = self.redis_client.get(key)
        return json.loads(value) if value else None

    def set(self, key: str, value, expire: int = 300):
        self.redis_client.setex(
            key,
            expire,
            json.dumps(value)
        )
```

ì‚¬ìš©:
```python
from app.services.cache_service import CacheService

cache = CacheService()

@router.get("/workflows/{id}")
def get_workflow(id: UUID, db: Session = Depends(get_db)):
    # ìºì‹œ í™•ì¸
    cache_key = f"workflow:{id}"
    cached = cache.get(cache_key)
    if cached:
        return cached

    # DB ì¡°íšŒ
    workflow = db.query(Workflow).filter(Workflow.id == id).first()
    if not workflow:
        raise HTTPException(status_code=404)

    # ìºì‹œ ì €ìž¥
    cache.set(cache_key, workflow, expire=300)
    return workflow
```

---

## ë³´ì•ˆ ë² ìŠ¤íŠ¸ í”„ëž™í‹°ìŠ¤

### 1. SQL Injection ë°©ì§€

```python
# Bad: SQL Injection ìœ„í—˜
workflow_name = request.query_params.get('name')
workflows = db.execute(f"SELECT * FROM workflows WHERE name = '{workflow_name}'")

# Good: íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
workflows = db.query(Workflow).filter(Workflow.name == workflow_name).all()
```

### 2. ìž…ë ¥ ê²€ì¦

```python
from pydantic import BaseModel, Field, validator

class WorkflowCreate(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)

    @validator('name')
    def sanitize_name(cls, v):
        # XSS ë°©ì§€: HTML íƒœê·¸ ì œê±°
        import html
        return html.escape(v)
```

### 3. í™˜ê²½ ë³€ìˆ˜ë¡œ ë¯¼ê° ì •ë³´ ê´€ë¦¬

```python
# Bad: ì½”ë“œì— í•˜ë“œì½”ë”©
API_KEY = "sk-1234567890abcdef"

# Good: í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
from app.core.config import settings
API_KEY = settings.API_KEY
```

### 4. CORS ì„¤ì •

```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # ê°œë°œí™˜ê²½
        "https://yourdomain.com"  # í”„ë¡œë•ì…˜
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)
```

---

## ìœ ìš©í•œ ì½”ë“œ ìŠ¤ë‹ˆíŽ«

### 1. íŠ¸ëžœìž­ì…˜ ê´€ë¦¬

```python
from sqlalchemy.exc import SQLAlchemyError

@router.post("/complex-operation")
def complex_operation(db: Session = Depends(get_db)):
    try:
        # ì—¬ëŸ¬ DB ìž‘ì—…
        workflow = Workflow(name="test")
        db.add(workflow)
        db.flush()  # ID ìƒì„±í•˜ì§€ë§Œ ì»¤ë°‹ ì•ˆ í•¨

        task = Task(workflow_id=workflow.id, name="task1")
        db.add(task)

        db.commit()  # ëª¨ë“  ìž‘ì—… ì»¤ë°‹
        return {"message": "Success"}

    except SQLAlchemyError as e:
        db.rollback()  # ì—ëŸ¬ ë°œìƒ ì‹œ ë¡¤ë°±
        raise HTTPException(status_code=500, detail=str(e))
```

### 2. íŒŒì¼ ì—…ë¡œë“œ

```python
from fastapi import File, UploadFile

@router.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    contents = await file.read()

    # íŒŒì¼ ì €ìž¥
    with open(f"uploads/{file.filename}", "wb") as f:
        f.write(contents)

    return {"filename": file.filename, "size": len(contents)}
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
from fastapi.responses import StreamingResponse
import asyncio

async def generate_logs():
    for i in range(100):
        yield f"data: Log line {i}\n\n"
        await asyncio.sleep(0.1)

@router.get("/stream-logs")
async def stream_logs():
    return StreamingResponse(
        generate_logs(),
        media_type="text/event-stream"
    )
```

---

## ìžì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

### Q: ë¹„ë™ê¸°(async) vs ë™ê¸°(sync) ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?

**A:**
- **ë¹„ë™ê¸° ì‚¬ìš©**: I/O ë°”ìš´ë“œ ìž‘ì—… (HTTP ìš”ì²­, íŒŒì¼ ì½ê¸°/ì“°ê¸°)
- **ë™ê¸° ì‚¬ìš©**: CPU ë°”ìš´ë“œ ìž‘ì—…, SQLAlchemy ORM ì¿¼ë¦¬

```python
# ë¹„ë™ê¸° ì˜ˆì œ
@router.get("/external-api")
async def call_external_api():
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.example.com")
        return response.json()

# ë™ê¸° ì˜ˆì œ
@router.get("/workflows")
def list_workflows(db: Session = Depends(get_db)):
    return db.query(Workflow).all()
```

### Q: Pydantic vs SQLAlchemy ëª¨ë¸ì˜ ì°¨ì´ëŠ”?

**A:**
- **Pydantic (schemas/)**: API ìš”ì²­/ì‘ë‹µ ê²€ì¦, ì§ë ¬í™”
- **SQLAlchemy (models/)**: ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì •ì˜, ORM

```python
# SQLAlchemy ëª¨ë¸ (DB)
class Workflow(Base):
    __tablename__ = "workflows"
    id = Column(UUID, primary_key=True)
    name = Column(String)

# Pydantic ìŠ¤í‚¤ë§ˆ (API)
class WorkflowResponse(BaseModel):
    id: str
    name: str

    class Config:
        from_attributes = True  # SQLAlchemy ëª¨ë¸ â†’ Pydantic
```

### Q: ë§ˆì´ê·¸ë ˆì´ì…˜ ì—†ì´ í…Œì´ë¸”ì„ ë§Œë“¤ ìˆ˜ ìžˆë‚˜ìš”?

**A:** ê°€ëŠ¥í•˜ì§€ë§Œ ê¶Œìž¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```python
# ê°œë°œ/í…ŒìŠ¤íŠ¸ ìš©ë„ë¡œë§Œ ì‚¬ìš©
from app.core.database import engine, Base

Base.metadata.create_all(bind=engine)
```

í”„ë¡œë•ì…˜ì—ì„œëŠ” ë°˜ë“œì‹œ Alembic ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.

---

## ë‹¤ìŒ ë‹¨ê³„

1. **ì¸ì¦/ì¸ê°€ ì¶”ê°€**: JWT, OAuth2
2. **í…ŒìŠ¤íŠ¸ ìž‘ì„±**: Pytest, ì»¤ë²„ë¦¬ì§€ ì¸¡ì •
3. **API ë²„ì „ ê´€ë¦¬**: /api/v1, /api/v2
4. **ëª¨ë‹ˆí„°ë§**: Prometheus, Grafana
5. **ë¬¸ì„œ ìžë™í™”**: OpenAPI, Redoc

---

## ì°¸ê³  ìžë£Œ

- [FastAPI ê³µì‹ ë¬¸ì„œ](https://fastapi.tiangolo.com/)
- [SQLAlchemy íŠœí† ë¦¬ì–¼](https://docs.sqlalchemy.org/en/20/tutorial/)
- [Alembic ê°€ì´ë“œ](https://alembic.sqlalchemy.org/en/latest/tutorial.html)
- [Pydantic ë¬¸ì„œ](https://docs.pydantic.dev/latest/)

---

Backend ê°œë°œì„ ì¦ê²ê²Œ! ðŸš€
